\chapter{Discussões}
\label{chp:discussions}

A seguir serão apresentadas discussões sobre os resultados apresentados no capítulo \ref{chp:results}, bem como limitações do sistema proposto neste estudo, e trabalhos futuros na área de busca de código-fonte por linguagem natural. Por fim, serão relacionadas as contribuições do presente estudo.

No experimento 1, houveram amostras onde todas as palavras retiradas geraram praticamente o mesmo valor de acurácia, ou ainda amostras onde a redução de acurácia aconteceu em stop-words ou outras palavras não tão relevantes para aquele comentário em questão. Com isso, tendo em mente que a entrada do modelo de comparação são dois \textit{embeddings} com tamanho 768, e gerados por redes \textit{transformers} complexas, é possível que o modelo de comparação, o qual é propositalmente mais simples que os geradores de \textit{embeddings} utilizados, não tenha conseguido generalizar todas informações semânticas contidas nos embeddings de entrada.

Entretando, os resultados do experimento 1 também mostraram que, para parte das amostras, a retirada de palavras importantes do comentário reduziu notavelmente a similaridade obtida, o que indica que a rede conseguiu captar o significado semântico destas palavras retiradas no comentário original. Algumas amostras, porém, obtiveram diferenças de acurácia menores do que outras. Essa diferença menor de acurácia entre amostras também pode ser explicada, já que apesar de uma palavra importante estar sendo removida do comentário, o \textit{embedding} de código-fonte será idêntico para todas as queries $C^-$ geradas para essa amostra, o que significa que ao menos 50\% da entrada do modelo de comparação será igual. Portanto, diferenças substanciais da similaridade, apenas removendo uma palavra do comentário original, mostra o potencial do modelo de comparação proposto neste estudo.

No experimento 2, os valores obtidos nas métricas \gls{mrr} e \textit{SuccessRate@k} foram baixos quando comparados diretamente com outros estudos presentes na literatura, conforme mostrado na Figura \ref{fig:results:experiment-2}. \textcite{Gu2018DeepCS}, por exemplo, obteve $0.367$ e $0.465$ nas métricas \textit{SuccessRate@1} e \gls{mrr}, respectivamente, enquanto \textcite{Gu2021CRaDLeDC} obteve $0.791$ e $0.843$ nas mesmas métricas, respectivamente. Entretanto, não é possível comparar diretamente tais resultados, já que os experimentos feitos no presente estudo diferem dos experimentos realizados pelos trabalhos citados. No presente estudo, foram utilizados apenas pares da linguagem \textit{Python}, e destes pares, 2000 amostras foram utilizadas tanto no treino quanto nesse experimento.

Por outro lado, obteve-se bons tempos de execução tanto para o treinamento quanto para a conclusão do experimento 2, sendo estes tempos de 48 minutos e 39 minutos e 45 segundos, respectivamente. Vale ressaltar que o modelo foi treinado sem \textit{hardware} dedicado (como placa de vídeo, por exemplo), e durante o desenvolvimento do presente estudo, bibliotecas como \textit{Keras} e \textit{TensorFlow} não estavam totalmente otimizadas para trabalhar com o processador \textit{Apple} M2 Pro, utilizado para desenvolvimento desse trabalho.

Por fim, o experimento 3 foi feito utilizando queries e valores de relevância disponíveis na base de dados \textit{CodeSearchNet}, conforme descrito na seção \ref{sec:experiments:experiment-3}. Aqui, utilizou-se tanto as queries disponíveis na base em questão, como suas respectivas relevâncias em relação a determinado trecho de código-fonte, disponíveis na mesma base. Além disso, muitos trechos de código-fonte dessa base de queries não foram utilizados nos experimentos 1 e 2. Por conta desses fatores, não foi possível utilizar métricas de recuperação de informação como \gls{mrr} ou \textit{SuccessRate@k}. Com isso, para este experimento, foi utilizado a similaridade (resultante do modelo de comparação) e o valor de relevância para determinar se o resultado do modelo de comparação está correto ou não.

\section{Limitações e trabalhos futuros}
\label{sec:discussions:future-works}

Durante o desenvolvimento deste trabalho, foram encontradas limitações em relação ao sistema proposto. Uma das limitações foi o tamanho dos \textit{embeddings} gerados, tanto para comentário quanto para código-fonte. Os modelos de \textit{embeddings} utilizados no presente estudo geram \textit{embeddings} de tamanho fixo, a saber, 768: caso o tamanho da entrada desses modelos seja maior do que 768, o vetor resultante será truncado; caso contrário, será adicionado um valor neutro de \textit{padding} no final do \textit{embedding} para atingir o tamanho desejado. Portanto, apesar de quaisquer modelos de embedding poderem ser usados no sistema proposto, o modelo de comparação espera \textit{embeddings} de tamanho 768, tanto para comentário quanto para código-fonte.

Outra limitação é a topologia utilizada no modelo de comparação. Propositalmente, o modelo de comparação utilizou uma rede \gls{mlp}, a qual é mais simples tanto em relação à outras redes neurais (como \gls{lstm}, por exemplo), quanto em relação aos modelos de \textit{embedding} utilizados neste trabalho, os quais são baseados em redes \textit{transformers}. Foram dois motivos que levaram a utilização de uma topologia mais simples para o modelo de comparação: o primeiro foi os requisitos computacionais. Redes \gls{mlp}, por serem mais simples, requerem muito menos recursos computacionais para treinamento do que uma rede baseada em \textit{transformer}, por exemplo. O segundo motivo foi justamente analisar como uma topologia mais simples se comportaria com entradas geradas por modelos mais complexos.

Diante disso, uma recomendação para trabalhos futuros seria utilizar outras topologias para comparação de embeddings. Antes dos modelos \textit{transformers} serem o estado da arte na área de \gls{nlp}, esse posto era ocupado pelas redes recorrentes como \gls{lstm}, por exemplo. Portanto, seria interessante validar se tais redes, apesar de mais complexas e, portanto, demandarem mais recursos computacionais que o modelo de comparação proposto no presente estudo, podem generalizar melhor as informações contidas nos \textit{embeddings} e, com isso, gerarem melhores resultados na busca de código-fonte a partir de linguagem natural.

Além disso, no presente estudo utilizou-se apenas uma combinação de modelos de embeddings para comentário e código-fonte. Em trabalhos futuros, pode-se utilizar outros modelos de embedding, tanto para linguagem natural (comentário) quanto para código-fonte. Inclusive, tais modelos de embedding não precisam, necessariamente, serem baseados em transformers, embora hoje esses sejam o estado da arte. Modelos de embedding como \gls{nbow}, por exemplo, também podem ser utilizados no sistema em questão.

Ainda, cita-se também a possibilidade de, em trabalhos futuros, de ajustar os parâmetros da rede \gls{mlp} utilizada para o atual modelo de comparação, a fim de melhorar a performance do sistema. Parâmetros como número e tipo de camadas intermediárias, tamanho dos embeddings de entrada e funções de ativação (tanto das camadas intermediárias quanto da camada de saída), podem facilmente ser alterados a fim de aperfeiçoar o sistema proposto. Além disso, os resultados do experimento 1 apontam que quanto maior o tamanaho do comentário, menor a eficácia da rede em determinar palavras semanticamente relevantes, embora essa regra não se aplique a todas as amostras utilizadas. Com isso, a remoção de palavras não relevantes (\textit{stop words}) durante o treinamento do modelo de comparação pode também melhorar sua performance.

Outra possibilidade para trabalhos futuros seria utilizar redes mais complexas para o modelo de comparação. Como redes recorrentes eram o estado da arte antes do advento dos modelos \textit{transformers}, substituir a atual rede \gls{mlp} por uma outra topologia como \gls{lstm} pode melhorar os resultados obtidos no presente estudo.

Por fim, os resultados obtidos na presente pesquisa foram submetidos à revista Journal of Systems and Software.